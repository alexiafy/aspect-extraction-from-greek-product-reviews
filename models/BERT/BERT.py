# -*- coding: utf-8 -*-
"""ae_BERT_tf_final_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dNDMUz_KoUtcsVK8IGP8sGzpabHONQpm
"""

import transformers

from transformers import BertTokenizer
from transformers import BertConfig
import numpy as np
import tqdm
import pandas as pd
import ast
from sklearn.model_selection import train_test_split
import tensorflow as tf
from transformers import BertTokenizer, TFBertModel, BertModel
from keras.preprocessing.sequence import pad_sequences
from sklearn.metrics import classification_report
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.layers import LSTM, Dropout, Bidirectional, GRU
from tensorflow.keras.regularizers import l2, l1
from keras.layers.merge import concatenate
from keras.layers.convolutional import Conv2D
import tensorflow.keras.backend as K
import matplotlib.pyplot as plt


# TO RUN ON GPU
import tensorflow as tf
from tensorflow.python.client import device_lib

from models.RNN.crf_layer.crf_layer import CRF


def tokenize_and_preserve_labels(sentence, text_labels):
    tokenized_sentence = []
    labels = []

    for word, label in zip(sentence, text_labels):
        # Tokenize the word and count # of subwords the word is broken into
        tokenized_word = tokenizer.tokenize(word)
        n_subwords = len(tokenized_word)

        # Add the tokenized word to the final tokenized word list
        tokenized_sentence.extend(tokenized_word)

        # Add the same label to the new list of labels `n_subwords` times
        labels.extend([label] * n_subwords)

    return tokenized_sentence, labels


def bert_encode(texts, max_len):
    all_tokens = []
    all_masks = []
    all_segments = []

    for text in texts:
        # text = tokenizer.tokenize(text)
        # print(text)
        text = text[:max_len - 2]
        # print(text)
        input_sequence = text  # ["[CLS]"] + text + ["[SEP]"]
        pad_len = max_len - len(
            input_sequence)  # maybe do this with tokenizer (https://www.kaggle.com/maunish/jigsaw-tensorflow-hub-vs-hugging-face)

        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len
        pad_masks = [1] * len(input_sequence) + [0] * pad_len
        segment_ids = [0] * max_len

        all_tokens.append(tokens)
        all_masks.append(pad_masks)
        all_segments.append(segment_ids)

    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)


def build_model(bert_layer, use_CRF, use_BiGRU, concat_last_layers, sum_last_layers, max_len=512):

    input_word_ids = tf.keras.Input(shape=(max_length,), dtype=tf.int32, name="input_ids")
    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="input_mask")
    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name="segment_ids")


    # ------ Define BERT Layers ------
    if not concat_last_layers and not sum_last_layers:
        last_hidden_state = bert_layer(input_word_ids, input_mask, segment_ids)[
            0]  # use [0] to take the last hidden state & [2][1:] to get all the hidden states
        selected_hiddes_states = last_hidden_state

    if concat_last_layers:
        hidden_states = bert_layer(input_word_ids, input_mask, segment_ids)[2][
                        1:]  # use [0] to take the last hidden state & [2][1:] to get all the hidden states
        selected_hiddes_states = tf.keras.layers.Concatenate(axis=-1)(
            [hidden_states[i] for i in [-1, -2, -3, -4]])  # Middle layers: -5,-6,-7   Four last layers:-1,-2,-3,-4

    if sum_last_layers:
        hidden_states = bert_layer(input_word_ids, input_mask, segment_ids)[2][
                        1:]  # use [0] to take the last hidden state & [2][1:] to get all the hidden states
        selected_hiddes_states = tf.keras.layers.Add()(
            [hidden_states[i] for i in [-5, -6, -7]])  # Middle layers: -5,-6,-7   Four last layers:-1,-2,-3,-4

    temp_output = selected_hiddes_states



    # ------ Define Bidirectional Layers ------
    if use_BiGRU:
        x1 = Bidirectional(GRU(64, return_sequences=True, recurrent_dropout=0.3))(temp_output)
        temp_output = Bidirectional(GRU(64, return_sequences=True, recurrent_dropout=0.3))(x1)


    # ------ Define Regularizer Layer ------
    # temp_output = tf.keras.layers.Dense(256, kernel_regularizer=l2(0.001), activation="relu")(temp_output)


    # ------ Define Output Layers ------
    if use_CRF:

        crf = CRF(len(tag_values), learn_mode='marginal', unroll=True)
        output = crf(temp_output)

    else:
        output = tf.keras.layers.Dense(len(tag_values), activation='softmax')(temp_output)
        print(output.shape)

    if use_CRF:
        model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=output)
        model.compile(tf.keras.optimizers.Adam(lr=3e-5), loss=crf.loss_function,
                      metrics=['acc'])  # loss=crf.loss, metrics=[crf.accuracy])
    else:
        model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=output)
        model.compile(tf.keras.optimizers.Adam(lr=3e-5), loss='categorical_crossentropy', metrics=['accuracy'])

    return model


def plot_graphs(history, string):
  plt.plot(train_history.history[string])
  plt.plot(train_history.history['val_'+string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()



if __name__ == '__main__':
    print(device_lib.list_local_devices())
    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))


    # Bert model and tokenizer
    # --------------------------------------------
    model_name = 'nlpaueb/bert-base-greek-uncased-v1'
    tokenizer = BertTokenizer.from_pretrained(model_name)
    # --------------------------------------------


    # Read data
    # --------------------------------------------
    FOLDERNAME = "../../data/datasets/aspect_extraction_datasets/parts_1_2_3/"
    FILENAME = 'ae_parts_1_2_3_usrnames_replaced'
    tagging_system = 'IOB2_tagging'  # 'IOB2_sentiment_c3' #'IOB2_tagging'

    data = pd.read_csv(FOLDERNAME + FILENAME + ".csv")  # .head(20)
    data[tagging_system] = data[tagging_system].apply(
            lambda x: ast.literal_eval(x))  # convert IOB column to list

    # print(data)
    # --------------------------------------------


    # Prepare data
    # --------------------------------------------
    rows = [sent for sent in data[tagging_system]]
    sentences = [[item[0] for item in sent] for sent in rows]
    labels = [[item[1] for item in sent] for sent in rows]

    # print(rows[0])
    # print(sentences[0])
    # print(labels[0])
    # --------------------------------------------


    # Train test split
    # --------------------------------------------
    # split data into training and testing sets
    TEST_SIZE = 0.20
    X_train, X_test, Y_train, Y_test = train_test_split(sentences, labels, test_size=TEST_SIZE, random_state=8)
    print(len(X_train))
    print(len(X_test))
    print(len(Y_train))
    print(len(Y_test))
    # --------------------------------------------


    # Tokenize with BERT tokenizer and preserve the labels
    # --------------------------------------------
    max_length = 70

    tokenized_texts_and_labels = [
        tokenize_and_preserve_labels(sent, labs)
        for sent, labs in zip(X_train, Y_train)
    ]

    X_train_tok = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]
    Y_train_tok = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]

    X_train_enc = bert_encode(X_train_tok, max_length)

    tokenized_texts_and_labels = [
        tokenize_and_preserve_labels(sent, labs)
        for sent, labs in zip(X_test, Y_test)
    ]

    X_test_tok = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]
    Y_test_tok = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]

    X_test_enc = bert_encode(X_test_tok, max_length)
    # --------------------------------------------


    # --------------------------------------------
    # idx = 7
    # print(X_train_tok[idx])
    # print(Y_train_tok[idx])
    # --------------------------------------------


    # Encode train and test data
    # --------------------------------------------
    tag_values = list(set([word_tag_pair[1] for sent in rows for word_tag_pair in sent]))
    tag_values.append("PAD")
    tag2idx = {t: i for i, t in enumerate(tag_values)}
    print(tag_values)

    Y_train_encoded = [[tag2idx[tag] for tag in instance] for instance in Y_train_tok]
    Y_train_padded = pad_sequences(Y_train_encoded, maxlen=max_length, padding="post", truncating="post", value=tag2idx['PAD'])
    Y_train = to_categorical(Y_train_padded)


    Y_test_encoded = [[tag2idx[tag] for tag in instance] for instance in Y_test_tok]
    Y_test_padded = pad_sequences(Y_test_encoded, maxlen=max_length, padding="post", truncating="post", value=tag2idx['PAD'])
    Y_test = to_categorical(Y_test_padded)
    # --------------------------------------------


    # --------------------------------------------
    # tag_values = list(set([word_tag_pair[1] for sent in rows for word_tag_pair in sent]))
    # tag_values.append("PAD")
    # tag2idx = {t: i for i, t in enumerate(tag_values)}
    # --------------------------------------------


    # Define BERT model
    # --------------------------------------------
    config = BertConfig.from_pretrained(model_name, output_hidden_states=True)
    bert_layer = TFBertModel.from_pretrained(model_name, config=config)


    FREEZE_BASE_MODEL = False
    freeze_layer_count = -4 #-4 to freeze everything except from the last 4 layers

    # Freeze the base model weights.
    if FREEZE_BASE_MODEL:
        for layer in bert_layer.layers[:freeze_layer_count]:
            print(layer)
            layer.trainable = False
    # --------------------------------------------


    # Define and build model
    # --------------------------------------------
    use_CRF = False
    use_BiGRU = False
    concat_last_layers = False
    sum_last_layers = False

    model = build_model(bert_layer, use_CRF, use_BiGRU, concat_last_layers, sum_last_layers, max_len=max_length)
    model.summary()
    # --------------------------------------------


    # Train model
    # --------------------------------------------
    # if use_CRF:
    #  checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_viterbi_accuracy', save_best_only=True, verbose=1)
    #  earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_viterbi_accuracy', patience=3, verbose=1)
    # else:
    #  checkpoint = tf.keras.callbacks.ModelCheckpoint('model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)
    #  earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=1)


    # checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_freq="epoch",
    #                                     save_best_only=True, mode='max')

    earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=1)


    train_history = model.fit(
        X_train_enc, Y_train,
        validation_split=0.15,
        epochs=5,
        callbacks=[earlystopping],
        batch_size=32,
        verbose=1)
    # --------------------------------------------



    # Print accuracy and loss
    # --------------------------------------------
    loss, accuracy = model.evaluate(X_test_enc, Y_test, verbose=1)
    print("Loss: {0}, \nAccuracy: {1}".format(loss, accuracy))
    # --------------------------------------------


    # Predict on test data
    # --------------------------------------------
    y_pred = model.predict(X_test_enc, verbose=1)

    y_pred_processed = np.argmax(y_pred, axis=-1)
    y_test_true = np.argmax(Y_test, axis=-1)

    # print(y_pred_processed)
    # print(y_test_true)
    # --------------------------------------------


    # Print classification report
    # --------------------------------------------
    idx2tag = {i: w for w, i in tag2idx.items()}
    y_pred_tags = [[idx2tag[i] for i in row] for row in y_pred_processed]
    y_test_tags = [[idx2tag[i] for i in row] for row in y_test_true]

    labels = tag_values
    labels.remove('O')
    labels.remove('PAD')  # remove the 'O' tag in order to properly evaluate the model
    sorted_labels = sorted(
            labels,
            key=lambda name: (name[1:], name[0])
    )

    # print(sorted_labels)
    report = classification_report(np.concatenate(y_pred_tags), np.concatenate(y_test_tags), labels=sorted_labels, digits=4)
    print(report)

    # print(y_pred_tags)
    # --------------------------------------------

  
    # plot_graphs(train_history, "acc")
    # plot_graphs(train_history, "loss")

